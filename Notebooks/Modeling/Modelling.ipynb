{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 28008,
     "status": "ok",
     "timestamp": 1581407379680,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "vCmPu_7vEvyb",
    "outputId": "313f99a5-f7dd-4648-c1d0-1dfa434110a5"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgIO5wA9OVSS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OzZgu1msbkIn"
   },
   "source": [
    "## **Read data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMPaZja5ORoN"
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle(\"Selected_X_train.pkl\")\n",
    "X_test = pd.read_pickle(\"Selected_X_test.pkl\")\n",
    "y_train = pd.read_pickle(\"Selected_y_train.pkl\")\n",
    "y_test = pd.read_pickle(\"Selected_y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check same length of samples and labels\n",
    "# train\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "# test\n",
    "assert X_test.shape[0] == y_test.shape[0]\n",
    "\n",
    "# check same features\n",
    "assert X_train.shape[1] == X_test.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c-ATdp3UbtTA"
   },
   "source": [
    "## **Baseline prediction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 576,
     "status": "ok",
     "timestamp": 1581412885065,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "1qhRPHySi_dy",
    "outputId": "3208fd51-e78d-4199-88b7-85f1ae9f3004"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4742915570115163\n"
     ]
    }
   ],
   "source": [
    "y_baseline = np.full(y_test.shape,y_train.mean())\n",
    "print(f'{np.sqrt(mean_squared_error(y_test, y_baseline))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P-BSPmrbwSu"
   },
   "source": [
    "## **Linear Regression (Normalized)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zMz12E5SCZs"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(normalize=True).fit(X_train, y_train)\n",
    "y_pred_lin_reg_norm = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 564,
     "status": "ok",
     "timestamp": 1581412740941,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "zixEEEyIUwS9",
    "outputId": "91540a6d-d648-4088-c84f-7543a55c8f89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4646936533737782\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.sqrt(mean_squared_error(y_test, y_pred_lin_reg_norm))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ODvDP8Ykb2pr"
   },
   "source": [
    "## **Linear Regression (Un-Normalized)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2194,
     "status": "ok",
     "timestamp": 1581412771523,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "OhNMRZ5hi5Sz",
    "outputId": "5e9da3f9-b6b9-4119-dd4c-efb0be9e06cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4646936533737784\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression(normalize=False).fit(X_train, y_train)\n",
    "y_pred_lin_reg = reg.predict(X_test)\n",
    "print(f'{np.sqrt(mean_squared_error(y_test, y_pred_lin_reg))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jFt6VdG8b5vO"
   },
   "source": [
    "## **ElasticNet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 144
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 234697,
     "status": "ok",
     "timestamp": 1581421605141,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "wwCkZ_vXaMBi",
    "outputId": "554911b3-aeb7-4971-c31b-37d90b843e79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:   25.0s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Use randomized search to tune the parameters:\n",
    "\n",
    "params = {\"l1_ratio\": np.arange(0,1.01,0.05),\n",
    "                  \"alpha\": [0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30,100,300],\n",
    "                  \"warm_start\": [True,False],\n",
    "                  \"selection\": ['random', 'cyclic']}\n",
    "\n",
    "eNet = ElasticNet()\n",
    "elastic_net_rnd = RandomizedSearchCV(eNet, params, scoring='r2',\n",
    "                                     n_jobs = -1, verbose = 2)\n",
    "elastic_net_rnd.fit(X_train, y_train)\n",
    "y_pred_elasticnet = elastic_net_rnd.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 849,
     "status": "ok",
     "timestamp": 1581421673463,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "hPTsoKrvn8YO",
    "outputId": "6098d66f-8a69-4d3b-90ff-70e9a20de116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4636898895535482\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.sqrt(mean_squared_error(y_test, y_pred_elasticnet))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v13TD48mcAjD"
   },
   "source": [
    "## **Random Forest Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1430714,
     "status": "ok",
     "timestamp": 1581423178001,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "YcNZYLxW9r24",
    "outputId": "904e6439-b234-4c97-f83c-35069035343a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n"
     ]
    }
   ],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "# Consider using tree pruning\n",
    "rf = RandomForestRegressor()\n",
    "params = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': np.arange(2,50,7),\n",
    "    'max_features': np.arange(5,115,10),\n",
    "    'min_samples_leaf': np.arange(2,10,1),\n",
    "    'min_samples_split': np.arange(2,70,5),\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}\n",
    "rnd_forest_src = RandomizedSearchCV(rf, params, scoring='r2', n_jobs = -1, verbose = 2)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "rnd_forest_src.fit(X_train, y_train)\n",
    "print(rnd_forest_src.best_params_)\n",
    "best_grid = rnd_forest_src.best_estimator_\n",
    "y_pred_rnd_forest = rnd_forest_src.predict(X_test)\n",
    "print(f'{np.sqrt(mean_squared_error(y_test, y_pred_rnd_forest))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knoS9aadcHEy"
   },
   "source": [
    "## **XGBoost Regressor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3481123,
     "status": "ok",
     "timestamp": 1581427154239,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "2jSX7EguCROS",
    "outputId": "100b1a4c-6c3c-43d6-cb2a-78377865fdd4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "/usr/local/lib/python3.6/dist-packages/xgboost/core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n"
     ]
    }
   ],
   "source": [
    "# Various hyper-parameters to tune\n",
    "xgb = XGBRegressor()\n",
    "params = {'objective':['reg:squarederror'],\n",
    "              'learning_rate': [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10],\n",
    "              'max_depth': np.arange(2,50,7),\n",
    "              'subsample': [1], # Reduce to prevent overfitting\n",
    "              'colsample_bytree': [1]} # Reduct to prevent overfitting\n",
    "\n",
    "xgb_rnd = RandomizedSearchCV(xgb, params, scoring='r2', verbose = 0)\n",
    "\n",
    "xgb_rnd.fit(X_train,y_train)\n",
    "\n",
    "print(xgb_rnd.best_score_)\n",
    "print(xgb_rnd.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 714,
     "status": "ok",
     "timestamp": 1581427157430,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "8aeVb_CxKAC4",
    "outputId": "d2b98cf8-54b8-4f4c-ea6c-87a3005f69cd"
   },
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb_rnd.predict(X_test)\n",
    "print(f'{np.sqrt(mean_squared_error(y_test, y_pred_xgb))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gzb9woJscMaS"
   },
   "source": [
    "## **Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2994,
     "status": "ok",
     "timestamp": 1581427217343,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "61EchF8nJ8O_",
    "outputId": "a9c49033-af99-449a-8bbb-e3989127dc8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Embedding, TimeDistributed, recurrent\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LFRbwzjLKAuT"
   },
   "outputs": [],
   "source": [
    "# Neural network\n",
    "nn = Sequential()\n",
    "nn.add(Dense(128, input_dim=X_train.shape[1], activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "nn.add(Dense(64, activation='relu',kernel_regularizer=regularizers.l2(0.0001)))\n",
    "nn.add(Dense(1,activation='linear', kernel_regularizer=regularizers.l2(0.0001)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1581430029359,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "L6zdM3faUu74",
    "outputId": "95b2c6ea-3d37-4582-e36e-5e2997b8a3a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               24064     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 32,385\n",
      "Trainable params: 32,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YZbElH9-UwOZ"
   },
   "outputs": [],
   "source": [
    "nn.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12884,
     "status": "ok",
     "timestamp": 1581430048562,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "I-1Q6z8FU583",
    "outputId": "54ac9620-e648-4580-e85e-12ce41f07795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59556 samples, validate on 14889 samples\n",
      "Epoch 1/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0528 - val_loss: 2.2812\n",
      "Epoch 2/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0479 - val_loss: 2.2778\n",
      "Epoch 3/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0410 - val_loss: 2.2732\n",
      "Epoch 4/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0407 - val_loss: 2.2897\n",
      "Epoch 5/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0362 - val_loss: 2.2721\n",
      "Epoch 6/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0333 - val_loss: 2.3261\n",
      "Epoch 7/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0290 - val_loss: 2.2995\n",
      "Epoch 8/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0271 - val_loss: 2.3029\n",
      "Epoch 9/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0206 - val_loss: 2.3041\n",
      "Epoch 10/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0195 - val_loss: 2.3097\n",
      "Epoch 11/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0154 - val_loss: 2.2942\n",
      "Epoch 12/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0138 - val_loss: 2.3300\n",
      "Epoch 13/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0141 - val_loss: 2.2895\n",
      "Epoch 14/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0051 - val_loss: 2.3445\n",
      "Epoch 15/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0036 - val_loss: 2.2869\n",
      "Epoch 16/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 2.0002 - val_loss: 2.3228\n",
      "Epoch 17/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9975 - val_loss: 2.3344\n",
      "Epoch 18/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9931 - val_loss: 2.3284\n",
      "Epoch 19/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9922 - val_loss: 2.3541\n",
      "Epoch 20/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9909 - val_loss: 2.3275\n",
      "Epoch 21/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9854 - val_loss: 2.3241\n",
      "Epoch 22/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9796 - val_loss: 2.3832\n",
      "Epoch 23/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9808 - val_loss: 2.3904\n",
      "Epoch 24/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9783 - val_loss: 2.3725\n",
      "Epoch 25/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9781 - val_loss: 2.3313\n",
      "Epoch 26/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9720 - val_loss: 2.3606\n",
      "Epoch 27/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9722 - val_loss: 2.3455\n",
      "Epoch 28/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9703 - val_loss: 2.3687\n",
      "Epoch 29/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9692 - val_loss: 2.3888\n",
      "Epoch 30/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9647 - val_loss: 2.4064\n",
      "Epoch 31/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9603 - val_loss: 2.3752\n",
      "Epoch 32/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9635 - val_loss: 2.3704\n",
      "Epoch 33/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9537 - val_loss: 2.5015\n",
      "Epoch 34/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9559 - val_loss: 2.4165\n",
      "Epoch 35/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9508 - val_loss: 2.4331\n",
      "Epoch 36/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9528 - val_loss: 2.4049\n",
      "Epoch 37/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9448 - val_loss: 2.4232\n",
      "Epoch 38/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.9443 - val_loss: 2.4752\n",
      "Epoch 39/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.9416 - val_loss: 2.4359\n",
      "Epoch 40/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9350 - val_loss: 2.4565\n",
      "Epoch 41/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9391 - val_loss: 2.4374\n",
      "Epoch 42/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9349 - val_loss: 2.3852\n",
      "Epoch 43/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9320 - val_loss: 2.4184\n",
      "Epoch 44/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9312 - val_loss: 2.4825\n",
      "Epoch 45/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9375 - val_loss: 2.4666\n",
      "Epoch 46/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9310 - val_loss: 2.4674\n",
      "Epoch 47/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9222 - val_loss: 2.4224\n",
      "Epoch 48/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9237 - val_loss: 2.4793\n",
      "Epoch 49/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9309 - val_loss: 2.4529\n",
      "Epoch 50/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9209 - val_loss: 2.4694\n",
      "Epoch 51/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9236 - val_loss: 2.4473\n",
      "Epoch 52/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9214 - val_loss: 2.4719\n",
      "Epoch 53/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9229 - val_loss: 2.4841\n",
      "Epoch 54/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9151 - val_loss: 2.4521\n",
      "Epoch 55/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9132 - val_loss: 2.4523\n",
      "Epoch 56/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9059 - val_loss: 2.5005\n",
      "Epoch 57/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9065 - val_loss: 2.4986\n",
      "Epoch 58/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9070 - val_loss: 2.4751\n",
      "Epoch 59/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9119 - val_loss: 2.4563\n",
      "Epoch 60/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9061 - val_loss: 2.4701\n",
      "Epoch 61/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9046 - val_loss: 2.4984\n",
      "Epoch 62/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8985 - val_loss: 2.5709\n",
      "Epoch 63/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.9021 - val_loss: 2.5316\n",
      "Epoch 64/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8981 - val_loss: 2.4606\n",
      "Epoch 65/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8977 - val_loss: 2.5210\n",
      "Epoch 66/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8965 - val_loss: 2.5067\n",
      "Epoch 67/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8950 - val_loss: 2.5548\n",
      "Epoch 68/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8905 - val_loss: 2.5250\n",
      "Epoch 69/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8964 - val_loss: 2.4831\n",
      "Epoch 70/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8930 - val_loss: 2.4793\n",
      "Epoch 71/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8824 - val_loss: 2.5135\n",
      "Epoch 72/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8898 - val_loss: 2.5248\n",
      "Epoch 73/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8852 - val_loss: 2.6041\n",
      "Epoch 74/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8883 - val_loss: 2.5329\n",
      "Epoch 75/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8868 - val_loss: 2.4863\n",
      "Epoch 76/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8817 - val_loss: 2.4536\n",
      "Epoch 77/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8869 - val_loss: 2.5337\n",
      "Epoch 78/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8788 - val_loss: 2.5441\n",
      "Epoch 79/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8790 - val_loss: 2.5257\n",
      "Epoch 80/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8777 - val_loss: 2.4576\n",
      "Epoch 81/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8836 - val_loss: 2.5914\n",
      "Epoch 82/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8710 - val_loss: 2.5240\n",
      "Epoch 83/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8794 - val_loss: 2.6258\n",
      "Epoch 84/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8684 - val_loss: 2.5462\n",
      "Epoch 85/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8728 - val_loss: 2.5140\n",
      "Epoch 86/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8735 - val_loss: 2.5687\n",
      "Epoch 87/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8751 - val_loss: 2.5245\n",
      "Epoch 88/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8681 - val_loss: 2.6013\n",
      "Epoch 89/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8760 - val_loss: 2.5271\n",
      "Epoch 90/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8721 - val_loss: 2.5617\n",
      "Epoch 91/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8709 - val_loss: 2.5411\n",
      "Epoch 92/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8666 - val_loss: 2.5099\n",
      "Epoch 93/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8680 - val_loss: 2.5087\n",
      "Epoch 94/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8615 - val_loss: 2.6816\n",
      "Epoch 95/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8673 - val_loss: 2.5927\n",
      "Epoch 96/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8591 - val_loss: 2.5616\n",
      "Epoch 97/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8664 - val_loss: 2.5529\n",
      "Epoch 98/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8640 - val_loss: 2.5395\n",
      "Epoch 99/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8622 - val_loss: 2.5352\n",
      "Epoch 100/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8604 - val_loss: 2.5827\n",
      "Epoch 101/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8575 - val_loss: 2.5618\n",
      "Epoch 102/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8558 - val_loss: 2.5401\n",
      "Epoch 103/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8582 - val_loss: 2.6235\n",
      "Epoch 104/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8537 - val_loss: 2.5572\n",
      "Epoch 105/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8589 - val_loss: 2.5531\n",
      "Epoch 106/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8541 - val_loss: 2.5699\n",
      "Epoch 107/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8540 - val_loss: 2.5146\n",
      "Epoch 108/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8486 - val_loss: 2.5576\n",
      "Epoch 109/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8503 - val_loss: 2.6167\n",
      "Epoch 110/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8550 - val_loss: 2.5467\n",
      "Epoch 111/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8489 - val_loss: 2.6315\n",
      "Epoch 112/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8521 - val_loss: 2.5293\n",
      "Epoch 113/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8468 - val_loss: 2.6676\n",
      "Epoch 114/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8515 - val_loss: 2.5257\n",
      "Epoch 115/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8432 - val_loss: 2.5548\n",
      "Epoch 116/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8449 - val_loss: 2.7005\n",
      "Epoch 117/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8439 - val_loss: 2.5917\n",
      "Epoch 118/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8480 - val_loss: 2.5949\n",
      "Epoch 119/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8507 - val_loss: 2.5581\n",
      "Epoch 120/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8440 - val_loss: 2.5781\n",
      "Epoch 121/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8396 - val_loss: 2.6443\n",
      "Epoch 122/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8369 - val_loss: 2.6409\n",
      "Epoch 123/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8416 - val_loss: 2.6813\n",
      "Epoch 124/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8402 - val_loss: 2.5633\n",
      "Epoch 125/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8431 - val_loss: 2.6013\n",
      "Epoch 126/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8398 - val_loss: 2.6804\n",
      "Epoch 127/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8349 - val_loss: 2.6161\n",
      "Epoch 128/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8334 - val_loss: 2.6459\n",
      "Epoch 129/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8337 - val_loss: 2.6092\n",
      "Epoch 130/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8354 - val_loss: 2.5954\n",
      "Epoch 131/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8339 - val_loss: 2.6109\n",
      "Epoch 132/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8344 - val_loss: 2.6200\n",
      "Epoch 133/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8403 - val_loss: 2.7000\n",
      "Epoch 134/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8332 - val_loss: 2.6411\n",
      "Epoch 135/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8264 - val_loss: 2.6481\n",
      "Epoch 136/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8333 - val_loss: 2.6500\n",
      "Epoch 137/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8337 - val_loss: 2.7077\n",
      "Epoch 138/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8345 - val_loss: 2.5997\n",
      "Epoch 139/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8257 - val_loss: 2.6247\n",
      "Epoch 140/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8326 - val_loss: 2.6145\n",
      "Epoch 141/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8318 - val_loss: 2.6341\n",
      "Epoch 142/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8239 - val_loss: 2.6108\n",
      "Epoch 143/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8295 - val_loss: 2.6790\n",
      "Epoch 144/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8237 - val_loss: 2.6571\n",
      "Epoch 145/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8233 - val_loss: 2.6448\n",
      "Epoch 146/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8309 - val_loss: 2.6034\n",
      "Epoch 147/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8250 - val_loss: 2.5521\n",
      "Epoch 148/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8273 - val_loss: 2.7018\n",
      "Epoch 149/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8173 - val_loss: 2.6532\n",
      "Epoch 150/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8245 - val_loss: 2.6550\n",
      "Epoch 151/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8253 - val_loss: 2.6867\n",
      "Epoch 152/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8173 - val_loss: 2.6161\n",
      "Epoch 153/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8168 - val_loss: 2.6703\n",
      "Epoch 154/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8251 - val_loss: 2.6227\n",
      "Epoch 155/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8187 - val_loss: 2.6081\n",
      "Epoch 156/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8243 - val_loss: 2.5840\n",
      "Epoch 157/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8218 - val_loss: 2.6213\n",
      "Epoch 158/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8172 - val_loss: 2.6510\n",
      "Epoch 159/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8133 - val_loss: 2.6266\n",
      "Epoch 160/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8155 - val_loss: 2.6685\n",
      "Epoch 161/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8191 - val_loss: 2.6426\n",
      "Epoch 162/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8127 - val_loss: 2.6276\n",
      "Epoch 163/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8276 - val_loss: 2.6422\n",
      "Epoch 164/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8136 - val_loss: 2.6153\n",
      "Epoch 165/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8166 - val_loss: 2.6798\n",
      "Epoch 166/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8240 - val_loss: 2.6455\n",
      "Epoch 167/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8152 - val_loss: 2.6410\n",
      "Epoch 168/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8141 - val_loss: 2.6268\n",
      "Epoch 169/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8148 - val_loss: 2.6461\n",
      "Epoch 170/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8135 - val_loss: 2.6363\n",
      "Epoch 171/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8221 - val_loss: 2.6573\n",
      "Epoch 172/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8057 - val_loss: 2.6234\n",
      "Epoch 173/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8090 - val_loss: 2.6921\n",
      "Epoch 174/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8147 - val_loss: 2.6351\n",
      "Epoch 175/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8145 - val_loss: 2.6178\n",
      "Epoch 176/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8104 - val_loss: 2.6190\n",
      "Epoch 177/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8142 - val_loss: 2.6866\n",
      "Epoch 178/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8126 - val_loss: 2.6871\n",
      "Epoch 179/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8141 - val_loss: 2.6070\n",
      "Epoch 180/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8051 - val_loss: 2.7385\n",
      "Epoch 181/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8115 - val_loss: 2.6621\n",
      "Epoch 182/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8046 - val_loss: 2.6281\n",
      "Epoch 183/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8004 - val_loss: 2.6979\n",
      "Epoch 184/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8099 - val_loss: 2.6147\n",
      "Epoch 185/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8116 - val_loss: 2.6692\n",
      "Epoch 186/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8074 - val_loss: 2.6981\n",
      "Epoch 187/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8084 - val_loss: 2.7614\n",
      "Epoch 188/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8030 - val_loss: 2.7013\n",
      "Epoch 189/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8131 - val_loss: 2.6848\n",
      "Epoch 190/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8085 - val_loss: 2.6594\n",
      "Epoch 191/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8017 - val_loss: 2.6659\n",
      "Epoch 192/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8091 - val_loss: 2.7062\n",
      "Epoch 193/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8097 - val_loss: 2.6871\n",
      "Epoch 194/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8022 - val_loss: 2.6772\n",
      "Epoch 195/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8094 - val_loss: 2.7708\n",
      "Epoch 196/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.8047 - val_loss: 2.6751\n",
      "Epoch 197/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8009 - val_loss: 2.7213\n",
      "Epoch 198/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8145 - val_loss: 2.6812\n",
      "Epoch 199/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8118 - val_loss: 2.7079\n",
      "Epoch 200/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7995 - val_loss: 2.7200\n",
      "Epoch 201/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8082 - val_loss: 2.6943\n",
      "Epoch 202/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8011 - val_loss: 2.6297\n",
      "Epoch 203/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8033 - val_loss: 2.6653\n",
      "Epoch 204/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8089 - val_loss: 2.6617\n",
      "Epoch 205/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7969 - val_loss: 2.6913\n",
      "Epoch 206/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8079 - val_loss: 2.6154\n",
      "Epoch 207/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8081 - val_loss: 2.6482\n",
      "Epoch 208/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8021 - val_loss: 2.6350\n",
      "Epoch 209/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7985 - val_loss: 2.7140\n",
      "Epoch 210/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8079 - val_loss: 2.6148\n",
      "Epoch 211/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7966 - val_loss: 2.6199\n",
      "Epoch 212/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8060 - val_loss: 2.6543\n",
      "Epoch 213/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7996 - val_loss: 2.6556\n",
      "Epoch 214/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8032 - val_loss: 2.6631\n",
      "Epoch 215/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7967 - val_loss: 2.7100\n",
      "Epoch 216/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7962 - val_loss: 2.6107\n",
      "Epoch 217/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8021 - val_loss: 2.6531\n",
      "Epoch 218/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8016 - val_loss: 2.6624\n",
      "Epoch 219/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7958 - val_loss: 2.6748\n",
      "Epoch 220/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7910 - val_loss: 2.6156\n",
      "Epoch 221/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7931 - val_loss: 2.6165\n",
      "Epoch 222/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7902 - val_loss: 2.6683\n",
      "Epoch 223/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7894 - val_loss: 2.6641\n",
      "Epoch 224/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7896 - val_loss: 2.6469\n",
      "Epoch 225/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7919 - val_loss: 2.6511\n",
      "Epoch 226/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7998 - val_loss: 2.7332\n",
      "Epoch 227/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7927 - val_loss: 2.6613\n",
      "Epoch 228/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7914 - val_loss: 2.6512\n",
      "Epoch 229/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7896 - val_loss: 2.6401\n",
      "Epoch 230/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7927 - val_loss: 2.6087\n",
      "Epoch 231/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7928 - val_loss: 2.6489\n",
      "Epoch 232/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7863 - val_loss: 2.6595\n",
      "Epoch 233/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7963 - val_loss: 2.6852\n",
      "Epoch 234/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7903 - val_loss: 2.6343\n",
      "Epoch 235/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7850 - val_loss: 2.6534\n",
      "Epoch 236/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7977 - val_loss: 2.6524\n",
      "Epoch 237/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7898 - val_loss: 2.6265\n",
      "Epoch 238/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7848 - val_loss: 2.7512\n",
      "Epoch 239/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7902 - val_loss: 2.6778\n",
      "Epoch 240/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7905 - val_loss: 2.7279\n",
      "Epoch 241/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7848 - val_loss: 2.7175\n",
      "Epoch 242/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7848 - val_loss: 2.7541\n",
      "Epoch 243/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7964 - val_loss: 2.6640\n",
      "Epoch 244/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7913 - val_loss: 2.7441\n",
      "Epoch 245/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7905 - val_loss: 2.6869\n",
      "Epoch 246/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7853 - val_loss: 2.6556\n",
      "Epoch 247/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7858 - val_loss: 2.6259\n",
      "Epoch 248/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7849 - val_loss: 2.6609\n",
      "Epoch 249/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7910 - val_loss: 2.7004\n",
      "Epoch 250/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7812 - val_loss: 2.7013\n",
      "Epoch 251/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7909 - val_loss: 2.6644\n",
      "Epoch 252/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7868 - val_loss: 2.6410\n",
      "Epoch 253/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7851 - val_loss: 2.7009\n",
      "Epoch 254/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7844 - val_loss: 2.6346\n",
      "Epoch 255/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7865 - val_loss: 2.6787\n",
      "Epoch 256/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7771 - val_loss: 2.7709\n",
      "Epoch 257/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7909 - val_loss: 2.6938\n",
      "Epoch 258/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7934 - val_loss: 2.6809\n",
      "Epoch 259/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7815 - val_loss: 2.7262\n",
      "Epoch 260/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7823 - val_loss: 2.6639\n",
      "Epoch 261/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7882 - val_loss: 2.6860\n",
      "Epoch 262/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7842 - val_loss: 2.6966\n",
      "Epoch 263/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7840 - val_loss: 2.6679\n",
      "Epoch 264/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7826 - val_loss: 2.7445\n",
      "Epoch 265/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7937 - val_loss: 2.6548\n",
      "Epoch 266/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7888 - val_loss: 2.6632\n",
      "Epoch 267/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7826 - val_loss: 2.6868\n",
      "Epoch 268/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7928 - val_loss: 2.6558\n",
      "Epoch 269/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7907 - val_loss: 2.6232\n",
      "Epoch 270/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7829 - val_loss: 2.6863\n",
      "Epoch 271/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7783 - val_loss: 2.6332\n",
      "Epoch 272/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7817 - val_loss: 2.7902\n",
      "Epoch 273/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7879 - val_loss: 2.6709\n",
      "Epoch 274/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7825 - val_loss: 2.7266\n",
      "Epoch 275/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7823 - val_loss: 2.6739\n",
      "Epoch 276/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7804 - val_loss: 2.7543\n",
      "Epoch 277/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7791 - val_loss: 2.6922\n",
      "Epoch 278/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7746 - val_loss: 2.7600\n",
      "Epoch 279/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7814 - val_loss: 2.6300\n",
      "Epoch 280/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7785 - val_loss: 2.7463\n",
      "Epoch 281/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7847 - val_loss: 2.6864\n",
      "Epoch 282/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7820 - val_loss: 2.6833\n",
      "Epoch 283/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7828 - val_loss: 2.6757\n",
      "Epoch 284/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7933 - val_loss: 2.6313\n",
      "Epoch 285/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7701 - val_loss: 2.6954\n",
      "Epoch 286/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7797 - val_loss: 2.6239\n",
      "Epoch 287/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7766 - val_loss: 2.6982\n",
      "Epoch 288/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7794 - val_loss: 2.7682\n",
      "Epoch 289/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.8036 - val_loss: 2.6372\n",
      "Epoch 290/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7734 - val_loss: 2.7124\n",
      "Epoch 291/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7787 - val_loss: 2.7005\n",
      "Epoch 292/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7721 - val_loss: 2.7446\n",
      "Epoch 293/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7818 - val_loss: 2.6942\n",
      "Epoch 294/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7735 - val_loss: 2.6444\n",
      "Epoch 295/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7784 - val_loss: 2.6609\n",
      "Epoch 296/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7732 - val_loss: 2.6914\n",
      "Epoch 297/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7798 - val_loss: 2.6984\n",
      "Epoch 298/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7754 - val_loss: 2.6832\n",
      "Epoch 299/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7766 - val_loss: 2.7166\n",
      "Epoch 300/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7812 - val_loss: 2.6759\n",
      "Epoch 301/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7828 - val_loss: 2.7350\n",
      "Epoch 302/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7737 - val_loss: 2.6580\n",
      "Epoch 303/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7799 - val_loss: 2.6518\n",
      "Epoch 304/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7718 - val_loss: 2.7376\n",
      "Epoch 305/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7843 - val_loss: 2.7251\n",
      "Epoch 306/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7764 - val_loss: 2.7745\n",
      "Epoch 307/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7827 - val_loss: 2.6783\n",
      "Epoch 308/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7688 - val_loss: 2.6823\n",
      "Epoch 309/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7771 - val_loss: 2.7451\n",
      "Epoch 310/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7769 - val_loss: 2.7826\n",
      "Epoch 311/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7776 - val_loss: 2.6757\n",
      "Epoch 312/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7728 - val_loss: 2.7150\n",
      "Epoch 313/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7706 - val_loss: 2.6887\n",
      "Epoch 314/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7787 - val_loss: 2.8187\n",
      "Epoch 315/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7926 - val_loss: 2.6406\n",
      "Epoch 316/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7948 - val_loss: 2.6946\n",
      "Epoch 317/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7802 - val_loss: 2.7505\n",
      "Epoch 318/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7812 - val_loss: 2.6706\n",
      "Epoch 319/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7741 - val_loss: 2.6877\n",
      "Epoch 320/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7682 - val_loss: 2.7351\n",
      "Epoch 321/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7834 - val_loss: 2.7305\n",
      "Epoch 322/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7744 - val_loss: 2.7009\n",
      "Epoch 323/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7661 - val_loss: 2.8234\n",
      "Epoch 324/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7699 - val_loss: 2.6656\n",
      "Epoch 325/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7721 - val_loss: 2.6333\n",
      "Epoch 326/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7924 - val_loss: 2.6466\n",
      "Epoch 327/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7736 - val_loss: 2.7543\n",
      "Epoch 328/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7677 - val_loss: 2.7242\n",
      "Epoch 329/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7682 - val_loss: 2.7443\n",
      "Epoch 330/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7712 - val_loss: 2.7332\n",
      "Epoch 331/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7745 - val_loss: 2.6585\n",
      "Epoch 332/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7702 - val_loss: 2.6279\n",
      "Epoch 333/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7717 - val_loss: 2.7623\n",
      "Epoch 334/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7805 - val_loss: 2.6683\n",
      "Epoch 335/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7774 - val_loss: 2.6937\n",
      "Epoch 336/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7762 - val_loss: 2.6668\n",
      "Epoch 337/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7661 - val_loss: 2.7317\n",
      "Epoch 338/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7796 - val_loss: 2.7176\n",
      "Epoch 339/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7695 - val_loss: 2.7762\n",
      "Epoch 340/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7710 - val_loss: 2.7107\n",
      "Epoch 341/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7698 - val_loss: 2.6567\n",
      "Epoch 342/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7650 - val_loss: 2.7000\n",
      "Epoch 343/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7719 - val_loss: 2.7375\n",
      "Epoch 344/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7686 - val_loss: 2.7264\n",
      "Epoch 345/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7790 - val_loss: 2.6860\n",
      "Epoch 346/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7670 - val_loss: 2.6912\n",
      "Epoch 347/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7642 - val_loss: 2.6468\n",
      "Epoch 348/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7656 - val_loss: 2.6780\n",
      "Epoch 349/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7652 - val_loss: 2.6910\n",
      "Epoch 350/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7658 - val_loss: 2.7095\n",
      "Epoch 351/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7627 - val_loss: 2.7358\n",
      "Epoch 352/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7822 - val_loss: 2.7030\n",
      "Epoch 353/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7627 - val_loss: 2.6882\n",
      "Epoch 354/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7676 - val_loss: 2.6533\n",
      "Epoch 355/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7644 - val_loss: 2.6929\n",
      "Epoch 356/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7626 - val_loss: 2.6871\n",
      "Epoch 357/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7727 - val_loss: 2.7002\n",
      "Epoch 358/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7615 - val_loss: 2.7624\n",
      "Epoch 359/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7637 - val_loss: 2.7022\n",
      "Epoch 360/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7651 - val_loss: 2.6808\n",
      "Epoch 361/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7664 - val_loss: 2.7223\n",
      "Epoch 362/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7746 - val_loss: 2.6737\n",
      "Epoch 363/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7703 - val_loss: 2.6606\n",
      "Epoch 364/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7639 - val_loss: 2.6919\n",
      "Epoch 365/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7637 - val_loss: 2.7630\n",
      "Epoch 366/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7577 - val_loss: 2.6833\n",
      "Epoch 367/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7649 - val_loss: 2.7271\n",
      "Epoch 368/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7719 - val_loss: 2.7336\n",
      "Epoch 369/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7644 - val_loss: 2.7294\n",
      "Epoch 370/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7572 - val_loss: 2.7033\n",
      "Epoch 371/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7599 - val_loss: 2.6744\n",
      "Epoch 372/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7577 - val_loss: 2.7195\n",
      "Epoch 373/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7685 - val_loss: 2.7266\n",
      "Epoch 374/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7650 - val_loss: 2.6841\n",
      "Epoch 375/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7666 - val_loss: 2.6914\n",
      "Epoch 376/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7639 - val_loss: 2.6962\n",
      "Epoch 377/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7576 - val_loss: 2.6886\n",
      "Epoch 378/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7632 - val_loss: 2.6664\n",
      "Epoch 379/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7577 - val_loss: 2.6667\n",
      "Epoch 380/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7615 - val_loss: 2.7465\n",
      "Epoch 381/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7556 - val_loss: 2.7112\n",
      "Epoch 382/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7677 - val_loss: 2.7135\n",
      "Epoch 383/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7591 - val_loss: 2.7114\n",
      "Epoch 384/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7563 - val_loss: 2.7021\n",
      "Epoch 385/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7587 - val_loss: 2.7312\n",
      "Epoch 386/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7625 - val_loss: 2.6834\n",
      "Epoch 387/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7571 - val_loss: 2.7107\n",
      "Epoch 388/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7692 - val_loss: 2.7585\n",
      "Epoch 389/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7788 - val_loss: 2.7751\n",
      "Epoch 390/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7588 - val_loss: 2.7813\n",
      "Epoch 391/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7648 - val_loss: 2.8228\n",
      "Epoch 392/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7601 - val_loss: 2.7112\n",
      "Epoch 393/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7536 - val_loss: 2.7281\n",
      "Epoch 394/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7677 - val_loss: 2.6557\n",
      "Epoch 395/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7589 - val_loss: 2.7706\n",
      "Epoch 396/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7622 - val_loss: 2.7766\n",
      "Epoch 397/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7618 - val_loss: 2.7021\n",
      "Epoch 398/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7705 - val_loss: 2.6660\n",
      "Epoch 399/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7654 - val_loss: 2.6765\n",
      "Epoch 400/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7567 - val_loss: 2.6590\n",
      "Epoch 401/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7609 - val_loss: 2.7443\n",
      "Epoch 402/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7598 - val_loss: 2.7206\n",
      "Epoch 403/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7532 - val_loss: 2.6940\n",
      "Epoch 404/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7568 - val_loss: 2.7041\n",
      "Epoch 405/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7587 - val_loss: 2.7538\n",
      "Epoch 406/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7530 - val_loss: 2.7079\n",
      "Epoch 407/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7597 - val_loss: 2.7116\n",
      "Epoch 408/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7588 - val_loss: 2.7306\n",
      "Epoch 409/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7576 - val_loss: 2.7024\n",
      "Epoch 410/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7562 - val_loss: 2.6859\n",
      "Epoch 411/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7600 - val_loss: 2.7023\n",
      "Epoch 412/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7574 - val_loss: 2.7147\n",
      "Epoch 413/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7554 - val_loss: 2.6874\n",
      "Epoch 414/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7604 - val_loss: 2.7637\n",
      "Epoch 415/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7588 - val_loss: 2.7615\n",
      "Epoch 416/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7629 - val_loss: 2.6924\n",
      "Epoch 417/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7500 - val_loss: 2.7085\n",
      "Epoch 418/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7568 - val_loss: 2.7113\n",
      "Epoch 419/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7519 - val_loss: 2.7510\n",
      "Epoch 420/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7544 - val_loss: 2.7505\n",
      "Epoch 421/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7498 - val_loss: 2.6942\n",
      "Epoch 422/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7607 - val_loss: 2.6942\n",
      "Epoch 423/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7635 - val_loss: 2.7126\n",
      "Epoch 424/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7475 - val_loss: 2.7158\n",
      "Epoch 425/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7645 - val_loss: 2.7115\n",
      "Epoch 426/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7678 - val_loss: 2.6440\n",
      "Epoch 427/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7561 - val_loss: 2.6429\n",
      "Epoch 428/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7528 - val_loss: 2.7049\n",
      "Epoch 429/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7524 - val_loss: 2.7427\n",
      "Epoch 430/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7514 - val_loss: 2.7159\n",
      "Epoch 431/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7638 - val_loss: 2.6907\n",
      "Epoch 432/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7537 - val_loss: 2.7254\n",
      "Epoch 433/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7521 - val_loss: 2.6785\n",
      "Epoch 434/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7451 - val_loss: 2.8052\n",
      "Epoch 435/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7613 - val_loss: 2.7155\n",
      "Epoch 436/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7546 - val_loss: 2.7302\n",
      "Epoch 437/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7523 - val_loss: 2.7058\n",
      "Epoch 438/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7497 - val_loss: 2.6608\n",
      "Epoch 439/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7706 - val_loss: 2.7573\n",
      "Epoch 440/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7548 - val_loss: 2.7436\n",
      "Epoch 441/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7567 - val_loss: 2.6451\n",
      "Epoch 442/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7560 - val_loss: 2.7584\n",
      "Epoch 443/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7496 - val_loss: 2.8153\n",
      "Epoch 444/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7529 - val_loss: 2.7503\n",
      "Epoch 445/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7507 - val_loss: 2.6790\n",
      "Epoch 446/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7570 - val_loss: 2.7208\n",
      "Epoch 447/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7519 - val_loss: 2.7237\n",
      "Epoch 448/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7569 - val_loss: 2.6831\n",
      "Epoch 449/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7562 - val_loss: 2.6552\n",
      "Epoch 450/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7729 - val_loss: 2.6829\n",
      "Epoch 451/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7576 - val_loss: 2.6851\n",
      "Epoch 452/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7536 - val_loss: 2.6589\n",
      "Epoch 453/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7523 - val_loss: 2.6920\n",
      "Epoch 454/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7535 - val_loss: 2.7194\n",
      "Epoch 455/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7483 - val_loss: 2.7168\n",
      "Epoch 456/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7575 - val_loss: 2.7365\n",
      "Epoch 457/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7510 - val_loss: 2.6842\n",
      "Epoch 458/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7467 - val_loss: 2.7287\n",
      "Epoch 459/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7567 - val_loss: 2.7122\n",
      "Epoch 460/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7601 - val_loss: 2.7218\n",
      "Epoch 461/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7649 - val_loss: 2.7215\n",
      "Epoch 462/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7531 - val_loss: 2.8109\n",
      "Epoch 463/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7618 - val_loss: 2.7470\n",
      "Epoch 464/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7563 - val_loss: 2.7219\n",
      "Epoch 465/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7430 - val_loss: 2.6982\n",
      "Epoch 466/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7420 - val_loss: 2.7847\n",
      "Epoch 467/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7506 - val_loss: 2.6978\n",
      "Epoch 468/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7578 - val_loss: 2.7091\n",
      "Epoch 469/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7416 - val_loss: 2.7387\n",
      "Epoch 470/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7491 - val_loss: 2.6983\n",
      "Epoch 471/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7778 - val_loss: 2.7300\n",
      "Epoch 472/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7592 - val_loss: 2.7855\n",
      "Epoch 473/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7534 - val_loss: 2.7098\n",
      "Epoch 474/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7538 - val_loss: 2.7329\n",
      "Epoch 475/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7433 - val_loss: 2.7353\n",
      "Epoch 476/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7454 - val_loss: 2.7442\n",
      "Epoch 477/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7502 - val_loss: 2.8019\n",
      "Epoch 478/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7571 - val_loss: 2.7532\n",
      "Epoch 479/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7585 - val_loss: 2.7083\n",
      "Epoch 480/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7565 - val_loss: 2.7370\n",
      "Epoch 481/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7463 - val_loss: 2.7502\n",
      "Epoch 482/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7478 - val_loss: 2.7469\n",
      "Epoch 483/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7508 - val_loss: 2.6745\n",
      "Epoch 484/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7550 - val_loss: 2.7147\n",
      "Epoch 485/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7538 - val_loss: 2.8039\n",
      "Epoch 486/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7449 - val_loss: 2.7583\n",
      "Epoch 487/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7542 - val_loss: 2.6771\n",
      "Epoch 488/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7462 - val_loss: 2.6743\n",
      "Epoch 489/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7514 - val_loss: 2.7094\n",
      "Epoch 490/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7497 - val_loss: 2.7141\n",
      "Epoch 491/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7517 - val_loss: 2.7347\n",
      "Epoch 492/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7424 - val_loss: 2.7227\n",
      "Epoch 493/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7502 - val_loss: 2.6889\n",
      "Epoch 494/500\n",
      "59556/59556 [==============================] - 1s 22us/step - loss: 1.7585 - val_loss: 2.7432\n",
      "Epoch 495/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7594 - val_loss: 2.6592\n",
      "Epoch 496/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7454 - val_loss: 2.7076\n",
      "Epoch 497/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7452 - val_loss: 2.7551\n",
      "Epoch 498/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7487 - val_loss: 2.7475\n",
      "Epoch 499/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7404 - val_loss: 2.7092\n",
      "Epoch 500/500\n",
      "59556/59556 [==============================] - 1s 21us/step - loss: 1.7497 - val_loss: 2.7387\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f55053669e8>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.fit(X_train, y_train, epochs=500, batch_size=100,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-qZrzocVaRI"
   },
   "outputs": [],
   "source": [
    "y_pred_nn = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1581430088036,
     "user": {
      "displayName": "Yaniv Ben-Malka",
      "photoUrl": "",
      "userId": "05518310637569340711"
     },
     "user_tz": -120
    },
    "id": "abwfS4s4Vmfq",
    "outputId": "edddd31c-3c21-4c16-e985-5d90db4fe8ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6164601135317884\n"
     ]
    }
   ],
   "source": [
    "print(f'{np.sqrt(mean_squared_error(y_test, y_pred_nn))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_L1DquxbaTKE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
