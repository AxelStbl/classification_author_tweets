{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('reddit_jokes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.score > 0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 'joke' column and remove unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Now I have to say \"Leroy can you please paint ...</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate how you cant even say black paint anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A Sunday school teacher is concerned that his ...</td>\n",
       "      <td>5tz2wj</td>\n",
       "      <td>1</td>\n",
       "      <td>Brian raises his hand and says, “He’s in Heaven.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Apparently \"Whatever's low in cholesterol\" was...</td>\n",
       "      <td>5tz04j</td>\n",
       "      <td>1</td>\n",
       "      <td>I walked into a PETA adoption center and the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>I always thought they were gunna hit me, not t...</td>\n",
       "      <td>5tyzxh</td>\n",
       "      <td>15</td>\n",
       "      <td>Remember when you were a kid and when you crie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>I said, \"I'm not sure; it's hard to keep track.\"</td>\n",
       "      <td>5tyytx</td>\n",
       "      <td>3</td>\n",
       "      <td>My boss said to me, \"you're the worst train dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132987</th>\n",
       "      <td>194547</td>\n",
       "      <td>Just this morning she said, \"Daddy, is that th...</td>\n",
       "      <td>1a8a5r</td>\n",
       "      <td>123</td>\n",
       "      <td>My daughter has reached that age where she's a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132988</th>\n",
       "      <td>194548</td>\n",
       "      <td>Gives me something to read while i'm in the sh...</td>\n",
       "      <td>1a89ts</td>\n",
       "      <td>5</td>\n",
       "      <td>I like a girl with words tattooed on her back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132989</th>\n",
       "      <td>194549</td>\n",
       "      <td>I mean dyslexia fcuk!!! &gt;_&lt;</td>\n",
       "      <td>1a87we</td>\n",
       "      <td>12</td>\n",
       "      <td>I have sexdaily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132990</th>\n",
       "      <td>194550</td>\n",
       "      <td>A hockey player showers after three periods.</td>\n",
       "      <td>1a7xnd</td>\n",
       "      <td>44</td>\n",
       "      <td>What's the difference between a hippie chick a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132991</th>\n",
       "      <td>194551</td>\n",
       "      <td>A father buys a lie detector robot that slaps ...</td>\n",
       "      <td>1a813f</td>\n",
       "      <td>63</td>\n",
       "      <td>new family robot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132992 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                               body      id  \\\n",
       "0            0  Now I have to say \"Leroy can you please paint ...  5tz52q   \n",
       "1            3  A Sunday school teacher is concerned that his ...  5tz2wj   \n",
       "2            7  Apparently \"Whatever's low in cholesterol\" was...  5tz04j   \n",
       "3           10  I always thought they were gunna hit me, not t...  5tyzxh   \n",
       "4           13   I said, \"I'm not sure; it's hard to keep track.\"  5tyytx   \n",
       "...        ...                                                ...     ...   \n",
       "132987  194547  Just this morning she said, \"Daddy, is that th...  1a8a5r   \n",
       "132988  194548  Gives me something to read while i'm in the sh...  1a89ts   \n",
       "132989  194549                        I mean dyslexia fcuk!!! >_<  1a87we   \n",
       "132990  194550       A hockey player showers after three periods.  1a7xnd   \n",
       "132991  194551  A father buys a lie detector robot that slaps ...  1a813f   \n",
       "\n",
       "        score                                              title  \n",
       "0           1   I hate how you cant even say black paint anymore  \n",
       "1           1  Brian raises his hand and says, “He’s in Heaven.”  \n",
       "2           1  I walked into a PETA adoption center and the r...  \n",
       "3          15  Remember when you were a kid and when you crie...  \n",
       "4           3  My boss said to me, \"you're the worst train dr...  \n",
       "...       ...                                                ...  \n",
       "132987    123  My daughter has reached that age where she's a...  \n",
       "132988      5     I like a girl with words tattooed on her back.  \n",
       "132989     12                                 I have sexdaily...  \n",
       "132990     44  What's the difference between a hippie chick a...  \n",
       "132991     63                                   new family robot  \n",
       "\n",
       "[132992 rows x 5 columns]"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke'] = df['title'] +'. '+ df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,['joke','score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process text:\n",
    "- Remove punctuation\n",
    "- Replace escape characters\n",
    "- Remove extra spaces\n",
    "- Remove single characters\n",
    "- Remove prefixed 'b'\n",
    "- Lowercase all characters\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_eng_punct(txt):\n",
    "    return re.sub(r'/[^a-zA-Z0-9\\s,.?!]/','*',txt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_url(txt):\n",
    "#     return re.sub(r'https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_escape(txt):\n",
    "    updated_txt = re.sub(r'\\n|\\t|&amp;',' ',txt)\n",
    "    return updated_txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi_spaces(txt):\n",
    "    return re.sub(' +', ' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "#         # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "#         # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "#         # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "        return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke'] = df['joke'].apply(replace_non_eng_punct).apply(remove_multi_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_preprocessed'] = df['joke'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample before/after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hate how you cant even say black paint anymore. Now I have to say \"Leroy can you please paint the fence?\"'"
      ]
     },
     "execution_count": 460,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['joke'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hate cant even black paint anymore leroy please paint fence'"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['joke_preprocessed'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create joke tokens list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_punct(txt):\n",
    "    return re.split(r'(\\W)',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_tokens'] = df['joke_preprocessed'].apply(split_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(lst):\n",
    "    return [x for x in lst if (x != ' ') and (x != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_tokens'] = df['joke_tokens'].apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'score'\n",
    "tokens_col = 'joke_preprocessed'\n",
    "X = df[tokens_col].to_numpy()\n",
    "y = df[target_col].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \\\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for s in df['joke_tokens']:\n",
    "    for w in s:\n",
    "        cnt[w] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = cnt.most_common(1000)\n",
    "vocab2 = [i[0] for i in vocab2]\n",
    "# vocab_id = defaultdict(int)\n",
    "# for ind,w in enumerate(vocab):\n",
    "#     vocab_id[w[0]] = ind+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate TFIDF/CountVectorizer features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_filter_vec2 = np.vectorize(lambda x: ' '.join([i for i in x.split(' ') if i in vocab2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-479-dd100c40f7c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_filter_vec2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_filter_vec2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2089\u001b[0m             \u001b[0mvargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_n\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_n\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2091\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vectorize_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2092\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2093\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ufunc_and_otypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py\u001b[0m in \u001b[0;36m_vectorize_call\u001b[1;34m(self, func, args)\u001b[0m\n\u001b[0;32m   2165\u001b[0m                       for a in args]\n\u001b[0;32m   2166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2167\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2169\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mufunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnout\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-478-aed6911b3f8d>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mconcat_filter_vec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvocab2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train = concat_filter_vec2(X_train)\n",
    "X_test = concat_filter_vec2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf = TfidfVectorizer()\n",
    "X_train_tf = vectorizer_tf.fit_transform(X_train).toarray()\n",
    "X_test_tf = vectorizer_tf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CountVectorizer()\n",
    "X_train_count = c.fit_transform(X_train).toarray()\n",
    "X_test_count = c.fit_transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Embedding, TimeDistributed, recurrent\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(1, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train_tf, y_train, epochs=20, batch_size=200,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
