{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models.fasttext import FastText\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk import WordPunctTokenizer\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('reddit_jokes.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.score > 0].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create 'joke' column and remove unnecessary columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>body</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Now I have to say \"Leroy can you please paint ...</td>\n",
       "      <td>5tz52q</td>\n",
       "      <td>1</td>\n",
       "      <td>I hate how you cant even say black paint anymore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>A Sunday school teacher is concerned that his ...</td>\n",
       "      <td>5tz2wj</td>\n",
       "      <td>1</td>\n",
       "      <td>Brian raises his hand and says, “He’s in Heaven.”</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>Apparently \"Whatever's low in cholesterol\" was...</td>\n",
       "      <td>5tz04j</td>\n",
       "      <td>1</td>\n",
       "      <td>I walked into a PETA adoption center and the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>I always thought they were gunna hit me, not t...</td>\n",
       "      <td>5tyzxh</td>\n",
       "      <td>15</td>\n",
       "      <td>Remember when you were a kid and when you crie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>I said, \"I'm not sure; it's hard to keep track.\"</td>\n",
       "      <td>5tyytx</td>\n",
       "      <td>3</td>\n",
       "      <td>My boss said to me, \"you're the worst train dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132987</th>\n",
       "      <td>194547</td>\n",
       "      <td>Just this morning she said, \"Daddy, is that th...</td>\n",
       "      <td>1a8a5r</td>\n",
       "      <td>123</td>\n",
       "      <td>My daughter has reached that age where she's a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132988</th>\n",
       "      <td>194548</td>\n",
       "      <td>Gives me something to read while i'm in the sh...</td>\n",
       "      <td>1a89ts</td>\n",
       "      <td>5</td>\n",
       "      <td>I like a girl with words tattooed on her back.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132989</th>\n",
       "      <td>194549</td>\n",
       "      <td>I mean dyslexia fcuk!!! &gt;_&lt;</td>\n",
       "      <td>1a87we</td>\n",
       "      <td>12</td>\n",
       "      <td>I have sexdaily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132990</th>\n",
       "      <td>194550</td>\n",
       "      <td>A hockey player showers after three periods.</td>\n",
       "      <td>1a7xnd</td>\n",
       "      <td>44</td>\n",
       "      <td>What's the difference between a hippie chick a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132991</th>\n",
       "      <td>194551</td>\n",
       "      <td>A father buys a lie detector robot that slaps ...</td>\n",
       "      <td>1a813f</td>\n",
       "      <td>63</td>\n",
       "      <td>new family robot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132992 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                               body      id  \\\n",
       "0            0  Now I have to say \"Leroy can you please paint ...  5tz52q   \n",
       "1            3  A Sunday school teacher is concerned that his ...  5tz2wj   \n",
       "2            7  Apparently \"Whatever's low in cholesterol\" was...  5tz04j   \n",
       "3           10  I always thought they were gunna hit me, not t...  5tyzxh   \n",
       "4           13   I said, \"I'm not sure; it's hard to keep track.\"  5tyytx   \n",
       "...        ...                                                ...     ...   \n",
       "132987  194547  Just this morning she said, \"Daddy, is that th...  1a8a5r   \n",
       "132988  194548  Gives me something to read while i'm in the sh...  1a89ts   \n",
       "132989  194549                        I mean dyslexia fcuk!!! >_<  1a87we   \n",
       "132990  194550       A hockey player showers after three periods.  1a7xnd   \n",
       "132991  194551  A father buys a lie detector robot that slaps ...  1a813f   \n",
       "\n",
       "        score                                              title  \n",
       "0           1   I hate how you cant even say black paint anymore  \n",
       "1           1  Brian raises his hand and says, “He’s in Heaven.”  \n",
       "2           1  I walked into a PETA adoption center and the r...  \n",
       "3          15  Remember when you were a kid and when you crie...  \n",
       "4           3  My boss said to me, \"you're the worst train dr...  \n",
       "...       ...                                                ...  \n",
       "132987    123  My daughter has reached that age where she's a...  \n",
       "132988      5     I like a girl with words tattooed on her back.  \n",
       "132989     12                                 I have sexdaily...  \n",
       "132990     44  What's the difference between a hippie chick a...  \n",
       "132991     63                                   new family robot  \n",
       "\n",
       "[132992 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke'] = df['title'] +'. '+ df['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.loc[:,['joke','score']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process text:\n",
    "- Remove punctuation\n",
    "- Replace escape characters\n",
    "- Remove extra spaces\n",
    "- Remove single characters\n",
    "- Remove prefixed 'b'\n",
    "- Lowercase all characters\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_non_eng_punct(txt):\n",
    "    return re.sub(r'/[^a-zA-Z0-9\\s,.?!]/','*',txt).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_url(txt):\n",
    "#     return re.sub(r'https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,}','',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_escape(txt):\n",
    "    updated_txt = re.sub(r'\\n|\\t|&amp;',' ',txt)\n",
    "    return updated_txt.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_multi_spaces(txt):\n",
    "    return re.sub(' +', ' ',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(document):\n",
    "#         # Remove all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(document))\n",
    "\n",
    "#         # remove all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "#         # Remove single characters from the start\n",
    "        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "        # Substituting multiple spaces with single space\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "        # Removing prefixed 'b'\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "        # Converting to Lowercase\n",
    "        document = document.lower()\n",
    "\n",
    "        # Lemmatization\n",
    "        tokens = document.split()\n",
    "        tokens = [stemmer.lemmatize(word) for word in tokens]\n",
    "        tokens = [word for word in tokens if word not in en_stop]\n",
    "        tokens = [word for word in tokens if len(word) > 3]\n",
    "\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "\n",
    "        return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke'] = df['joke'].apply(replace_non_eng_punct).apply(remove_multi_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_preprocessed'] = df['joke'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample before/after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I hate how you cant even say black paint anymore. Now I have to say \"Leroy can you please paint the fence?\"'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['joke'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hate cant even black paint anymore leroy please paint fence'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['joke_preprocessed'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create joke tokens list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_punct(txt):\n",
    "    return re.split(r'(\\W)',txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_tokens'] = df['joke_preprocessed'].apply(split_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_spaces(lst):\n",
    "    return [x for x in lst if (x != ' ') and (x != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['joke_tokens'] = df['joke_tokens'].apply(remove_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'score'\n",
    "tokens_col = 'joke_preprocessed'\n",
    "X = df[tokens_col].to_numpy()\n",
    "y = df[target_col].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, \\\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(y_train.reshape(-1,1))\n",
    "y_train = scaler.transform(y_train.reshape(-1,1)).reshape(-1,)\n",
    "y_test = scaler.transform(y_test.reshape(-1,1)).reshape(-1,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter()\n",
    "for s in df['joke_tokens']:\n",
    "    for w in s:\n",
    "        cnt[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = cnt.most_common(1000)\n",
    "vocab2 = [i[0] for i in vocab2]\n",
    "# vocab_id = defaultdict(int)\n",
    "# for ind,w in enumerate(vocab):\n",
    "#     vocab_id[w[0]] = ind+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate TFIDF/CountVectorizer features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat_filter_vec2 = np.vectorize(lambda x: ' '.join([i for i in x.split(' ') if i in vocab2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = concat_filter_vec2(X_train)\n",
    "X_test = concat_filter_vec2(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scientist came inside fuck dude hell away'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_tf = TfidfVectorizer()\n",
    "X_train_tf = vectorizer_tf.fit_transform(X_train).toarray()\n",
    "X_test_tf = vectorizer_tf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CountVectorizer()\n",
    "X_train_count = c.fit_transform(X_train).toarray()\n",
    "X_test_count = c.fit_transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.33728993, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106393, 1000)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Embedding, TimeDistributed, recurrent\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=1000, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(128, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(1, activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               128128    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 161,281\n",
      "Trainable params: 161,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 85114 samples, validate on 21279 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\yaniv\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "85114/85114 [==============================] - 5s 54us/step - loss: 4.4947 - mean_squared_error: 5.1584e-04 - val_loss: 4.0598 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 2/20\n",
      "85114/85114 [==============================] - 4s 42us/step - loss: 3.7326 - mean_squared_error: 5.1399e-04 - val_loss: 3.4221 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 3/20\n",
      "85114/85114 [==============================] - 3s 38us/step - loss: 3.1471 - mean_squared_error: 5.1399e-04 - val_loss: 2.8858 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 4/20\n",
      "85114/85114 [==============================] - 3s 40us/step - loss: 2.6539 - mean_squared_error: 5.1399e-04 - val_loss: 2.4336 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 5/20\n",
      "85114/85114 [==============================] - 4s 43us/step - loss: 2.2381 - mean_squared_error: 5.1399e-04 - val_loss: 2.0524 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 6/20\n",
      "85114/85114 [==============================] - 3s 38us/step - loss: 1.8875 - mean_squared_error: 5.1399e-04 - val_loss: 1.7309 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 7/20\n",
      "85114/85114 [==============================] - 3s 41us/step - loss: 1.5918 - mean_squared_error: 5.1399e-04 - val_loss: 1.4598 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 8/20\n",
      "85114/85114 [==============================] - 4s 42us/step - loss: 1.3425 - mean_squared_error: 5.1399e-04 - val_loss: 1.2312 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 9/20\n",
      "85114/85114 [==============================] - 4s 43us/step - loss: 1.1322 - mean_squared_error: 5.1399e-04 - val_loss: 1.0384 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 10/20\n",
      "85114/85114 [==============================] - 4s 43us/step - loss: 0.9549 - mean_squared_error: 5.1399e-04 - val_loss: 0.8758 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 11/20\n",
      "85114/85114 [==============================] - 4s 42us/step - loss: 0.8054 - mean_squared_error: 5.1399e-04 - val_loss: 0.7387 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 12/20\n",
      "85114/85114 [==============================] - 4s 43us/step - loss: 0.6793 - mean_squared_error: 5.1399e-04 - val_loss: 0.6230 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 13/20\n",
      "85114/85114 [==============================] - 4s 42us/step - loss: 0.5729 - mean_squared_error: 5.1399e-04 - val_loss: 0.5255 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 14/20\n",
      "85114/85114 [==============================] - 4s 46us/step - loss: 0.4832 - mean_squared_error: 5.1399e-04 - val_loss: 0.4433 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 15/20\n",
      "85114/85114 [==============================] - 4s 49us/step - loss: 0.4076 - mean_squared_error: 5.1399e-04 - val_loss: 0.3739 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 16/20\n",
      "85114/85114 [==============================] - 8s 98us/step - loss: 0.3438 - mean_squared_error: 5.1399e-04 - val_loss: 0.3154 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 17/20\n",
      "85114/85114 [==============================] - 4s 45us/step - loss: 0.2900 - mean_squared_error: 5.1399e-04 - val_loss: 0.2661 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 18/20\n",
      "85114/85114 [==============================] - 4s 45us/step - loss: 0.2447 - mean_squared_error: 5.1399e-04 - val_loss: 0.2245 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 19/20\n",
      "85114/85114 [==============================] - 4s 45us/step - loss: 0.2064 - mean_squared_error: 5.1399e-04 - val_loss: 0.1894 - val_mean_squared_error: 6.4839e-04\n",
      "Epoch 20/20\n",
      "85114/85114 [==============================] - 4s 44us/step - loss: 0.1741 - mean_squared_error: 5.1399e-04 - val_loss: 0.1599 - val_mean_squared_error: 6.4839e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_tf, y_train, epochs=20, batch_size=200,validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.89592993e-03, 0.00000000e+00, 2.30808861e-03, ...,\n",
       "       0.00000000e+00, 2.26687275e-04, 2.06079341e-05])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
